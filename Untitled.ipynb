{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import zeros, newaxis\n",
    "\n",
    "#to plot the graphs\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.io import arff #To load .arff type file\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder #to lebel categorical variables\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn.preprocessing import MinMaxScaler #to normalize data\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #import confusion_matrix\n",
    "from sklearn.metrics import classification_report #import classification_report\n",
    "from sklearn import preprocessing #to nonlinear transformation\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 14980 observations\n"
     ]
    }
   ],
   "source": [
    "data = arff.loadarff('EEG_Eye_State.arff')\n",
    "training_data = pd.DataFrame(data[0])\n",
    "\n",
    "print(\"Training set has {} observations\".format(len(training_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6',\n",
      "       'F4', 'F8', 'AF4', 'eyeDetection'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(training_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF3</th>\n",
       "      <th>F7</th>\n",
       "      <th>F3</th>\n",
       "      <th>FC5</th>\n",
       "      <th>T7</th>\n",
       "      <th>P7</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>P8</th>\n",
       "      <th>T8</th>\n",
       "      <th>FC6</th>\n",
       "      <th>F4</th>\n",
       "      <th>F8</th>\n",
       "      <th>AF4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "      <td>14980.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4321.917777</td>\n",
       "      <td>4009.767694</td>\n",
       "      <td>4264.022433</td>\n",
       "      <td>4164.946326</td>\n",
       "      <td>4341.741075</td>\n",
       "      <td>4644.022379</td>\n",
       "      <td>4110.400160</td>\n",
       "      <td>4616.056904</td>\n",
       "      <td>4218.826610</td>\n",
       "      <td>4231.316200</td>\n",
       "      <td>4202.456900</td>\n",
       "      <td>4279.232774</td>\n",
       "      <td>4615.205336</td>\n",
       "      <td>4416.435832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2492.072174</td>\n",
       "      <td>45.941672</td>\n",
       "      <td>44.428052</td>\n",
       "      <td>5216.404632</td>\n",
       "      <td>34.738821</td>\n",
       "      <td>2924.789537</td>\n",
       "      <td>4600.926543</td>\n",
       "      <td>29.292603</td>\n",
       "      <td>2136.408523</td>\n",
       "      <td>38.050903</td>\n",
       "      <td>37.785981</td>\n",
       "      <td>41.544312</td>\n",
       "      <td>1208.369958</td>\n",
       "      <td>5891.285043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1030.770000</td>\n",
       "      <td>2830.770000</td>\n",
       "      <td>1040.000000</td>\n",
       "      <td>2453.330000</td>\n",
       "      <td>2089.740000</td>\n",
       "      <td>2768.210000</td>\n",
       "      <td>2086.150000</td>\n",
       "      <td>4567.180000</td>\n",
       "      <td>1357.950000</td>\n",
       "      <td>1816.410000</td>\n",
       "      <td>3273.330000</td>\n",
       "      <td>2257.950000</td>\n",
       "      <td>86.666700</td>\n",
       "      <td>1366.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4280.510000</td>\n",
       "      <td>3990.770000</td>\n",
       "      <td>4250.260000</td>\n",
       "      <td>4108.210000</td>\n",
       "      <td>4331.790000</td>\n",
       "      <td>4611.790000</td>\n",
       "      <td>4057.950000</td>\n",
       "      <td>4604.620000</td>\n",
       "      <td>4190.770000</td>\n",
       "      <td>4220.510000</td>\n",
       "      <td>4190.260000</td>\n",
       "      <td>4267.690000</td>\n",
       "      <td>4590.770000</td>\n",
       "      <td>4342.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4294.360000</td>\n",
       "      <td>4005.640000</td>\n",
       "      <td>4262.560000</td>\n",
       "      <td>4120.510000</td>\n",
       "      <td>4338.970000</td>\n",
       "      <td>4617.950000</td>\n",
       "      <td>4070.260000</td>\n",
       "      <td>4613.330000</td>\n",
       "      <td>4199.490000</td>\n",
       "      <td>4229.230000</td>\n",
       "      <td>4200.510000</td>\n",
       "      <td>4276.920000</td>\n",
       "      <td>4603.080000</td>\n",
       "      <td>4354.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4311.790000</td>\n",
       "      <td>4023.080000</td>\n",
       "      <td>4270.770000</td>\n",
       "      <td>4132.310000</td>\n",
       "      <td>4347.180000</td>\n",
       "      <td>4626.670000</td>\n",
       "      <td>4083.590000</td>\n",
       "      <td>4624.100000</td>\n",
       "      <td>4209.230000</td>\n",
       "      <td>4239.490000</td>\n",
       "      <td>4211.280000</td>\n",
       "      <td>4287.180000</td>\n",
       "      <td>4617.440000</td>\n",
       "      <td>4372.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>309231.000000</td>\n",
       "      <td>7804.620000</td>\n",
       "      <td>6880.510000</td>\n",
       "      <td>642564.000000</td>\n",
       "      <td>6474.360000</td>\n",
       "      <td>362564.000000</td>\n",
       "      <td>567179.000000</td>\n",
       "      <td>7264.100000</td>\n",
       "      <td>265641.000000</td>\n",
       "      <td>6674.360000</td>\n",
       "      <td>6823.080000</td>\n",
       "      <td>7002.560000</td>\n",
       "      <td>152308.000000</td>\n",
       "      <td>715897.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AF3            F7            F3            FC5            T7  \\\n",
       "count   14980.000000  14980.000000  14980.000000   14980.000000  14980.000000   \n",
       "mean     4321.917777   4009.767694   4264.022433    4164.946326   4341.741075   \n",
       "std      2492.072174     45.941672     44.428052    5216.404632     34.738821   \n",
       "min      1030.770000   2830.770000   1040.000000    2453.330000   2089.740000   \n",
       "25%      4280.510000   3990.770000   4250.260000    4108.210000   4331.790000   \n",
       "50%      4294.360000   4005.640000   4262.560000    4120.510000   4338.970000   \n",
       "75%      4311.790000   4023.080000   4270.770000    4132.310000   4347.180000   \n",
       "max    309231.000000   7804.620000   6880.510000  642564.000000   6474.360000   \n",
       "\n",
       "                  P7             O1            O2             P8  \\\n",
       "count   14980.000000   14980.000000  14980.000000   14980.000000   \n",
       "mean     4644.022379    4110.400160   4616.056904    4218.826610   \n",
       "std      2924.789537    4600.926543     29.292603    2136.408523   \n",
       "min      2768.210000    2086.150000   4567.180000    1357.950000   \n",
       "25%      4611.790000    4057.950000   4604.620000    4190.770000   \n",
       "50%      4617.950000    4070.260000   4613.330000    4199.490000   \n",
       "75%      4626.670000    4083.590000   4624.100000    4209.230000   \n",
       "max    362564.000000  567179.000000   7264.100000  265641.000000   \n",
       "\n",
       "                 T8           FC6            F4             F8            AF4  \n",
       "count  14980.000000  14980.000000  14980.000000   14980.000000   14980.000000  \n",
       "mean    4231.316200   4202.456900   4279.232774    4615.205336    4416.435832  \n",
       "std       38.050903     37.785981     41.544312    1208.369958    5891.285043  \n",
       "min     1816.410000   3273.330000   2257.950000      86.666700    1366.150000  \n",
       "25%     4220.510000   4190.260000   4267.690000    4590.770000    4342.050000  \n",
       "50%     4229.230000   4200.510000   4276.920000    4603.080000    4354.870000  \n",
       "75%     4239.490000   4211.280000   4287.180000    4617.440000    4372.820000  \n",
       "max     6674.360000   6823.080000   7002.560000  152308.000000  715897.000000  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_data.isnull().values.sum()) #get the total number of missing values in the Dataframe\n",
    "#training_data.isnull().sum() #number of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF3</th>\n",
       "      <th>F7</th>\n",
       "      <th>F3</th>\n",
       "      <th>FC5</th>\n",
       "      <th>T7</th>\n",
       "      <th>P7</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>P8</th>\n",
       "      <th>T8</th>\n",
       "      <th>FC6</th>\n",
       "      <th>F4</th>\n",
       "      <th>F8</th>\n",
       "      <th>AF4</th>\n",
       "      <th>eyeDetection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4329.23</td>\n",
       "      <td>4009.23</td>\n",
       "      <td>4289.23</td>\n",
       "      <td>4148.21</td>\n",
       "      <td>4350.26</td>\n",
       "      <td>4586.15</td>\n",
       "      <td>4096.92</td>\n",
       "      <td>4641.03</td>\n",
       "      <td>4222.05</td>\n",
       "      <td>4238.46</td>\n",
       "      <td>4211.28</td>\n",
       "      <td>4280.51</td>\n",
       "      <td>4635.90</td>\n",
       "      <td>4393.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4324.62</td>\n",
       "      <td>4004.62</td>\n",
       "      <td>4293.85</td>\n",
       "      <td>4148.72</td>\n",
       "      <td>4342.05</td>\n",
       "      <td>4586.67</td>\n",
       "      <td>4097.44</td>\n",
       "      <td>4638.97</td>\n",
       "      <td>4210.77</td>\n",
       "      <td>4226.67</td>\n",
       "      <td>4207.69</td>\n",
       "      <td>4279.49</td>\n",
       "      <td>4632.82</td>\n",
       "      <td>4384.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4327.69</td>\n",
       "      <td>4006.67</td>\n",
       "      <td>4295.38</td>\n",
       "      <td>4156.41</td>\n",
       "      <td>4336.92</td>\n",
       "      <td>4583.59</td>\n",
       "      <td>4096.92</td>\n",
       "      <td>4630.26</td>\n",
       "      <td>4207.69</td>\n",
       "      <td>4222.05</td>\n",
       "      <td>4206.67</td>\n",
       "      <td>4282.05</td>\n",
       "      <td>4628.72</td>\n",
       "      <td>4389.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4328.72</td>\n",
       "      <td>4011.79</td>\n",
       "      <td>4296.41</td>\n",
       "      <td>4155.90</td>\n",
       "      <td>4343.59</td>\n",
       "      <td>4582.56</td>\n",
       "      <td>4097.44</td>\n",
       "      <td>4630.77</td>\n",
       "      <td>4217.44</td>\n",
       "      <td>4235.38</td>\n",
       "      <td>4210.77</td>\n",
       "      <td>4287.69</td>\n",
       "      <td>4632.31</td>\n",
       "      <td>4396.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4326.15</td>\n",
       "      <td>4011.79</td>\n",
       "      <td>4292.31</td>\n",
       "      <td>4151.28</td>\n",
       "      <td>4347.69</td>\n",
       "      <td>4586.67</td>\n",
       "      <td>4095.90</td>\n",
       "      <td>4627.69</td>\n",
       "      <td>4210.77</td>\n",
       "      <td>4244.10</td>\n",
       "      <td>4212.82</td>\n",
       "      <td>4288.21</td>\n",
       "      <td>4632.82</td>\n",
       "      <td>4398.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AF3       F7       F3      FC5       T7       P7       O1       O2  \\\n",
       "0  4329.23  4009.23  4289.23  4148.21  4350.26  4586.15  4096.92  4641.03   \n",
       "1  4324.62  4004.62  4293.85  4148.72  4342.05  4586.67  4097.44  4638.97   \n",
       "2  4327.69  4006.67  4295.38  4156.41  4336.92  4583.59  4096.92  4630.26   \n",
       "3  4328.72  4011.79  4296.41  4155.90  4343.59  4582.56  4097.44  4630.77   \n",
       "4  4326.15  4011.79  4292.31  4151.28  4347.69  4586.67  4095.90  4627.69   \n",
       "\n",
       "        P8       T8      FC6       F4       F8      AF4  eyeDetection  \n",
       "0  4222.05  4238.46  4211.28  4280.51  4635.90  4393.85             0  \n",
       "1  4210.77  4226.67  4207.69  4279.49  4632.82  4384.10             0  \n",
       "2  4207.69  4222.05  4206.67  4282.05  4628.72  4389.23             0  \n",
       "3  4217.44  4235.38  4210.77  4287.69  4632.31  4396.41             0  \n",
       "4  4210.77  4244.10  4212.82  4288.21  4632.82  4398.46             0  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Labeling categorical object values\n",
    "\n",
    "label_encoding = LabelEncoder()\n",
    "\n",
    "training_data[\"eyeDetection\"] = label_encoding.fit_transform(training_data[\"eyeDetection\"])\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010702</td>\n",
       "      <td>0.236931</td>\n",
       "      <td>0.556326</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.515557</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.027383</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.498575</td>\n",
       "      <td>0.264230</td>\n",
       "      <td>0.426286</td>\n",
       "      <td>0.029886</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010687</td>\n",
       "      <td>0.236004</td>\n",
       "      <td>0.557117</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.513684</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.026619</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>0.263219</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.029865</td>\n",
       "      <td>0.004224</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010697</td>\n",
       "      <td>0.236416</td>\n",
       "      <td>0.557379</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.512514</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.023390</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.495197</td>\n",
       "      <td>0.262931</td>\n",
       "      <td>0.426610</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010701</td>\n",
       "      <td>0.237446</td>\n",
       "      <td>0.557556</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.514035</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.023579</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>0.497940</td>\n",
       "      <td>0.264086</td>\n",
       "      <td>0.427799</td>\n",
       "      <td>0.029862</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.237446</td>\n",
       "      <td>0.556854</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.514971</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.499735</td>\n",
       "      <td>0.264664</td>\n",
       "      <td>0.427909</td>\n",
       "      <td>0.029865</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.236004</td>\n",
       "      <td>0.555448</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.514503</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.497414</td>\n",
       "      <td>0.263796</td>\n",
       "      <td>0.426395</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.235283</td>\n",
       "      <td>0.554833</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.514035</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.018065</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>0.261342</td>\n",
       "      <td>0.424016</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010691</td>\n",
       "      <td>0.236416</td>\n",
       "      <td>0.554482</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.514152</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.496887</td>\n",
       "      <td>0.259897</td>\n",
       "      <td>0.423369</td>\n",
       "      <td>0.029795</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.237241</td>\n",
       "      <td>0.554131</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.514387</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.496780</td>\n",
       "      <td>0.261630</td>\n",
       "      <td>0.424882</td>\n",
       "      <td>0.029828</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.237343</td>\n",
       "      <td>0.554219</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.514152</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.015403</td>\n",
       "      <td>0.010732</td>\n",
       "      <td>0.496570</td>\n",
       "      <td>0.264664</td>\n",
       "      <td>0.425746</td>\n",
       "      <td>0.029896</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.237241</td>\n",
       "      <td>0.553517</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.513801</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.018065</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.496570</td>\n",
       "      <td>0.264086</td>\n",
       "      <td>0.424665</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010662</td>\n",
       "      <td>0.235590</td>\n",
       "      <td>0.551234</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.017872</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.495514</td>\n",
       "      <td>0.260331</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>0.029737</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.233734</td>\n",
       "      <td>0.549214</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.512282</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.010711</td>\n",
       "      <td>0.495092</td>\n",
       "      <td>0.258308</td>\n",
       "      <td>0.421099</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.234766</td>\n",
       "      <td>0.549916</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.512865</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.495936</td>\n",
       "      <td>0.259463</td>\n",
       "      <td>0.422720</td>\n",
       "      <td>0.029717</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.010649</td>\n",
       "      <td>0.236312</td>\n",
       "      <td>0.551409</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.513452</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.022819</td>\n",
       "      <td>0.010814</td>\n",
       "      <td>0.495831</td>\n",
       "      <td>0.260762</td>\n",
       "      <td>0.424990</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.235797</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.512749</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.495726</td>\n",
       "      <td>0.260909</td>\n",
       "      <td>0.423908</td>\n",
       "      <td>0.029717</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.010629</td>\n",
       "      <td>0.234663</td>\n",
       "      <td>0.549127</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.512514</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.496041</td>\n",
       "      <td>0.259320</td>\n",
       "      <td>0.420774</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.010619</td>\n",
       "      <td>0.234354</td>\n",
       "      <td>0.549478</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.513452</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.010767</td>\n",
       "      <td>0.496041</td>\n",
       "      <td>0.258018</td>\n",
       "      <td>0.420667</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.235075</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.513684</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.495409</td>\n",
       "      <td>0.258886</td>\n",
       "      <td>0.422937</td>\n",
       "      <td>0.029747</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.235797</td>\n",
       "      <td>0.550883</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.513568</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.494141</td>\n",
       "      <td>0.261052</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.029791</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.236209</td>\n",
       "      <td>0.551234</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.514387</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.495302</td>\n",
       "      <td>0.263796</td>\n",
       "      <td>0.424558</td>\n",
       "      <td>0.029812</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.010687</td>\n",
       "      <td>0.237653</td>\n",
       "      <td>0.551849</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.513919</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>0.498996</td>\n",
       "      <td>0.266109</td>\n",
       "      <td>0.424990</td>\n",
       "      <td>0.029862</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.239921</td>\n",
       "      <td>0.552375</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.512398</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>0.499948</td>\n",
       "      <td>0.265675</td>\n",
       "      <td>0.424990</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.239096</td>\n",
       "      <td>0.552024</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.511930</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>0.498575</td>\n",
       "      <td>0.263509</td>\n",
       "      <td>0.424340</td>\n",
       "      <td>0.029876</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.010677</td>\n",
       "      <td>0.236622</td>\n",
       "      <td>0.551322</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.513217</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.023390</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.499418</td>\n",
       "      <td>0.263509</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.029832</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.010664</td>\n",
       "      <td>0.236622</td>\n",
       "      <td>0.550883</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>0.514035</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.026619</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.500474</td>\n",
       "      <td>0.263652</td>\n",
       "      <td>0.424016</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.010646</td>\n",
       "      <td>0.236519</td>\n",
       "      <td>0.549741</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.512398</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.024910</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.497309</td>\n",
       "      <td>0.260331</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>0.029751</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.010636</td>\n",
       "      <td>0.235075</td>\n",
       "      <td>0.548776</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.510760</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.018636</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.494036</td>\n",
       "      <td>0.257151</td>\n",
       "      <td>0.420667</td>\n",
       "      <td>0.029677</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.235488</td>\n",
       "      <td>0.550092</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.511579</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.494036</td>\n",
       "      <td>0.258162</td>\n",
       "      <td>0.421314</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.010644</td>\n",
       "      <td>0.237343</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.010754</td>\n",
       "      <td>0.494880</td>\n",
       "      <td>0.259897</td>\n",
       "      <td>0.423152</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14950</th>\n",
       "      <td>0.010706</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.555097</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.513452</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.495726</td>\n",
       "      <td>0.259173</td>\n",
       "      <td>0.427692</td>\n",
       "      <td>0.029758</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14951</th>\n",
       "      <td>0.010691</td>\n",
       "      <td>0.242603</td>\n",
       "      <td>0.554570</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.512282</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.495302</td>\n",
       "      <td>0.258739</td>\n",
       "      <td>0.428556</td>\n",
       "      <td>0.029747</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14952</th>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.242910</td>\n",
       "      <td>0.554833</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.511930</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>0.497309</td>\n",
       "      <td>0.261196</td>\n",
       "      <td>0.428663</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.004224</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14953</th>\n",
       "      <td>0.010697</td>\n",
       "      <td>0.242808</td>\n",
       "      <td>0.554833</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.513217</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.022630</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.498257</td>\n",
       "      <td>0.262641</td>\n",
       "      <td>0.428663</td>\n",
       "      <td>0.029822</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14954</th>\n",
       "      <td>0.010701</td>\n",
       "      <td>0.242603</td>\n",
       "      <td>0.554659</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.513684</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.497201</td>\n",
       "      <td>0.260762</td>\n",
       "      <td>0.428880</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>0.004224</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14955</th>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.242498</td>\n",
       "      <td>0.554922</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.512282</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>0.020916</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.497309</td>\n",
       "      <td>0.260041</td>\n",
       "      <td>0.429312</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14956</th>\n",
       "      <td>0.010679</td>\n",
       "      <td>0.242086</td>\n",
       "      <td>0.554746</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.511463</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>0.497519</td>\n",
       "      <td>0.260331</td>\n",
       "      <td>0.428663</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14957</th>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.241467</td>\n",
       "      <td>0.553955</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.512633</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>0.259030</td>\n",
       "      <td>0.427367</td>\n",
       "      <td>0.029771</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14958</th>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.240643</td>\n",
       "      <td>0.554044</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.512982</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>0.022819</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.496358</td>\n",
       "      <td>0.258018</td>\n",
       "      <td>0.426610</td>\n",
       "      <td>0.029771</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14959</th>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.239509</td>\n",
       "      <td>0.554482</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.512514</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.496992</td>\n",
       "      <td>0.257297</td>\n",
       "      <td>0.427152</td>\n",
       "      <td>0.029731</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14960</th>\n",
       "      <td>0.010621</td>\n",
       "      <td>0.238477</td>\n",
       "      <td>0.553693</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.513217</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.029474</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>0.496675</td>\n",
       "      <td>0.255995</td>\n",
       "      <td>0.426718</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14961</th>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.238477</td>\n",
       "      <td>0.553166</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.513684</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.029092</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.496675</td>\n",
       "      <td>0.255562</td>\n",
       "      <td>0.425529</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14962</th>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.238787</td>\n",
       "      <td>0.553429</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.513217</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.026241</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>0.255274</td>\n",
       "      <td>0.425854</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14963</th>\n",
       "      <td>0.010611</td>\n",
       "      <td>0.237860</td>\n",
       "      <td>0.552815</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.513452</td>\n",
       "      <td>0.005140</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.021866</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>0.495619</td>\n",
       "      <td>0.254840</td>\n",
       "      <td>0.425961</td>\n",
       "      <td>0.029670</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14964</th>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.236931</td>\n",
       "      <td>0.551936</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.513801</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>0.254840</td>\n",
       "      <td>0.425639</td>\n",
       "      <td>0.029657</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14965</th>\n",
       "      <td>0.010616</td>\n",
       "      <td>0.236829</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.512398</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.023579</td>\n",
       "      <td>0.010798</td>\n",
       "      <td>0.496253</td>\n",
       "      <td>0.254984</td>\n",
       "      <td>0.425854</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14966</th>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.236312</td>\n",
       "      <td>0.552375</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.511112</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.023957</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.495831</td>\n",
       "      <td>0.254407</td>\n",
       "      <td>0.425746</td>\n",
       "      <td>0.029687</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14967</th>\n",
       "      <td>0.010588</td>\n",
       "      <td>0.236107</td>\n",
       "      <td>0.551760</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.511579</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>0.495831</td>\n",
       "      <td>0.253973</td>\n",
       "      <td>0.425529</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14968</th>\n",
       "      <td>0.010604</td>\n",
       "      <td>0.236416</td>\n",
       "      <td>0.551498</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.512749</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.025099</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.496358</td>\n",
       "      <td>0.254407</td>\n",
       "      <td>0.425746</td>\n",
       "      <td>0.029646</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14969</th>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.236312</td>\n",
       "      <td>0.552024</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.024528</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.496675</td>\n",
       "      <td>0.255562</td>\n",
       "      <td>0.425961</td>\n",
       "      <td>0.029694</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14970</th>\n",
       "      <td>0.010573</td>\n",
       "      <td>0.235695</td>\n",
       "      <td>0.551936</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.512865</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.021487</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>0.255418</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.029710</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14971</th>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0.550356</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.512514</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.023390</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.496041</td>\n",
       "      <td>0.254840</td>\n",
       "      <td>0.426395</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14972</th>\n",
       "      <td>0.010569</td>\n",
       "      <td>0.234251</td>\n",
       "      <td>0.549303</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.511930</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.027383</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.496570</td>\n",
       "      <td>0.254984</td>\n",
       "      <td>0.424882</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>0.010551</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.549654</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.511463</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.495936</td>\n",
       "      <td>0.254116</td>\n",
       "      <td>0.423584</td>\n",
       "      <td>0.029623</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14974</th>\n",
       "      <td>0.010544</td>\n",
       "      <td>0.232808</td>\n",
       "      <td>0.549478</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.511463</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.024150</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.494775</td>\n",
       "      <td>0.253685</td>\n",
       "      <td>0.424340</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14975</th>\n",
       "      <td>0.010546</td>\n",
       "      <td>0.233117</td>\n",
       "      <td>0.548863</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.511814</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.021677</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>0.495092</td>\n",
       "      <td>0.252961</td>\n",
       "      <td>0.423908</td>\n",
       "      <td>0.029606</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14976</th>\n",
       "      <td>0.010533</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.548776</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.511579</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>0.010732</td>\n",
       "      <td>0.494248</td>\n",
       "      <td>0.250505</td>\n",
       "      <td>0.421856</td>\n",
       "      <td>0.029586</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14977</th>\n",
       "      <td>0.010534</td>\n",
       "      <td>0.233220</td>\n",
       "      <td>0.549039</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.511695</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.020916</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.493297</td>\n",
       "      <td>0.249927</td>\n",
       "      <td>0.421531</td>\n",
       "      <td>0.029596</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14978</th>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.549829</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.511930</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.022819</td>\n",
       "      <td>0.010754</td>\n",
       "      <td>0.494775</td>\n",
       "      <td>0.251373</td>\n",
       "      <td>0.423476</td>\n",
       "      <td>0.029626</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.234561</td>\n",
       "      <td>0.551322</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.511695</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.026430</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>0.251950</td>\n",
       "      <td>0.424990</td>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14980 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.010702  0.236931  0.556326  0.002648  0.515557  0.005053  0.003558   \n",
       "1      0.010687  0.236004  0.557117  0.002649  0.513684  0.005054  0.003559   \n",
       "2      0.010697  0.236416  0.557379  0.002661  0.512514  0.005046  0.003558   \n",
       "3      0.010701  0.237446  0.557556  0.002660  0.514035  0.005043  0.003559   \n",
       "4      0.010692  0.237446  0.556854  0.002653  0.514971  0.005054  0.003556   \n",
       "5      0.010676  0.236004  0.555448  0.002656  0.514503  0.005056  0.003552   \n",
       "6      0.010671  0.235283  0.554833  0.002653  0.514035  0.005048  0.003546   \n",
       "7      0.010691  0.236416  0.554482  0.002640  0.514152  0.005044  0.003541   \n",
       "8      0.010692  0.237241  0.554131  0.002634  0.514387  0.005047  0.003548   \n",
       "9      0.010692  0.237343  0.554219  0.002638  0.514152  0.005043  0.003551   \n",
       "10     0.010692  0.237241  0.553517  0.002640  0.513801  0.005034  0.003542   \n",
       "11     0.010662  0.235590  0.551234  0.002629  0.513100  0.005040  0.003539   \n",
       "12     0.010634  0.233734  0.549214  0.002611  0.512282  0.005048  0.003546   \n",
       "13     0.010634  0.234766  0.549916  0.002608  0.512865  0.005048  0.003548   \n",
       "14     0.010649  0.236312  0.551409  0.002616  0.513452  0.005053  0.003546   \n",
       "15     0.010652  0.235797  0.550620  0.002616  0.512749  0.005053  0.003548   \n",
       "16     0.010629  0.234663  0.549127  0.002608  0.512514  0.005047  0.003554   \n",
       "17     0.010619  0.234354  0.549478  0.002607  0.513452  0.005047  0.003547   \n",
       "18     0.010647  0.235075  0.550620  0.002610  0.513684  0.005054  0.003538   \n",
       "19     0.010659  0.235797  0.550883  0.002608  0.513568  0.005058  0.003543   \n",
       "20     0.010654  0.236209  0.551234  0.002609  0.514387  0.005058  0.003550   \n",
       "21     0.010687  0.237653  0.551849  0.002623  0.513919  0.005056  0.003549   \n",
       "22     0.010709  0.239921  0.552375  0.002635  0.512398  0.005051  0.003546   \n",
       "23     0.010694  0.239096  0.552024  0.002631  0.511930  0.005053  0.003543   \n",
       "24     0.010677  0.236622  0.551322  0.002623  0.513217  0.005051  0.003541   \n",
       "25     0.010664  0.236622  0.550883  0.002617  0.514035  0.005050  0.003543   \n",
       "26     0.010646  0.236519  0.549741  0.002612  0.512398  0.005056  0.003542   \n",
       "27     0.010636  0.235075  0.548776  0.002606  0.510760  0.005051  0.003540   \n",
       "28     0.010634  0.235488  0.550092  0.002608  0.511579  0.005043  0.003542   \n",
       "29     0.010644  0.237343  0.552200  0.002620  0.513100  0.005046  0.003546   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14950  0.010706  0.242396  0.555097  0.002637  0.513452  0.005133  0.003507   \n",
       "14951  0.010691  0.242603  0.554570  0.002643  0.512282  0.005118  0.003505   \n",
       "14952  0.010694  0.242910  0.554833  0.002649  0.511930  0.005103  0.003505   \n",
       "14953  0.010697  0.242808  0.554833  0.002645  0.513217  0.005117  0.003514   \n",
       "14954  0.010701  0.242603  0.554659  0.002649  0.513684  0.005137  0.003518   \n",
       "14955  0.010699  0.242498  0.554922  0.002656  0.512282  0.005137  0.003520   \n",
       "14956  0.010679  0.242086  0.554746  0.002653  0.511463  0.005127  0.003523   \n",
       "14957  0.010659  0.241467  0.553955  0.002649  0.512633  0.005131  0.003524   \n",
       "14958  0.010659  0.240643  0.554044  0.002649  0.512982  0.005148  0.003525   \n",
       "14959  0.010647  0.239509  0.554482  0.002649  0.512514  0.005157  0.003531   \n",
       "14960  0.010621  0.238477  0.553693  0.002643  0.513217  0.005157  0.003536   \n",
       "14961  0.010617  0.238477  0.553166  0.002633  0.513684  0.005151  0.003533   \n",
       "14962  0.010622  0.238787  0.553429  0.002634  0.513217  0.005145  0.003533   \n",
       "14963  0.010611  0.237860  0.552815  0.002635  0.513452  0.005140  0.003532   \n",
       "14964  0.010607  0.236931  0.551936  0.002633  0.513801  0.005141  0.003535   \n",
       "14965  0.010616  0.236829  0.552200  0.002633  0.512398  0.005143  0.003544   \n",
       "14966  0.010601  0.236312  0.552375  0.002630  0.511112  0.005135  0.003543   \n",
       "14967  0.010588  0.236107  0.551760  0.002627  0.511579  0.005131  0.003535   \n",
       "14968  0.010604  0.236416  0.551498  0.002631  0.512749  0.005134  0.003541   \n",
       "14969  0.010597  0.236312  0.552024  0.002627  0.513100  0.005145  0.003547   \n",
       "14970  0.010573  0.235695  0.551936  0.002611  0.512865  0.005150  0.003536   \n",
       "14971  0.010571  0.234973  0.550356  0.002605  0.512514  0.005144  0.003533   \n",
       "14972  0.010569  0.234251  0.549303  0.002604  0.511930  0.005135  0.003537   \n",
       "14973  0.010551  0.233425  0.549654  0.002597  0.511463  0.005127  0.003524   \n",
       "14974  0.010544  0.232808  0.549478  0.002599  0.511463  0.005127  0.003515   \n",
       "14975  0.010546  0.233117  0.548863  0.002599  0.511814  0.005131  0.003519   \n",
       "14976  0.010533  0.233425  0.548776  0.002589  0.511579  0.005134  0.003517   \n",
       "14977  0.010534  0.233220  0.549039  0.002594  0.511695  0.005134  0.003516   \n",
       "14978  0.010558  0.233425  0.549829  0.002607  0.511930  0.005137  0.003529   \n",
       "14979  0.010568  0.234561  0.551322  0.002605  0.511695  0.005137  0.003544   \n",
       "\n",
       "             7         8         9         10        11        12        13  \\\n",
       "0      0.027383  0.010837  0.498575  0.264230  0.426286  0.029886  0.004237   \n",
       "1      0.026619  0.010795  0.496148  0.263219  0.426071  0.029865  0.004224   \n",
       "2      0.023390  0.010783  0.495197  0.262931  0.426610  0.029838  0.004231   \n",
       "3      0.023579  0.010820  0.497940  0.264086  0.427799  0.029862  0.004241   \n",
       "4      0.022437  0.010795  0.499735  0.264664  0.427909  0.029865  0.004244   \n",
       "5      0.018443  0.010763  0.497414  0.263796  0.426395  0.029835  0.004232   \n",
       "6      0.018065  0.010800  0.496148  0.261342  0.424016  0.029815  0.004216   \n",
       "7      0.017683  0.010775  0.496887  0.259897  0.423369  0.029795  0.004219   \n",
       "8      0.015214  0.010707  0.496780  0.261630  0.424882  0.029828  0.004232   \n",
       "9      0.015403  0.010732  0.496570  0.264664  0.425746  0.029896  0.004237   \n",
       "10     0.018065  0.010777  0.496570  0.264086  0.424665  0.029859  0.004222   \n",
       "11     0.017872  0.010738  0.495514  0.260331  0.422395  0.029737  0.004205   \n",
       "12     0.015974  0.010711  0.495092  0.258308  0.421099  0.029690  0.004204   \n",
       "13     0.018443  0.010775  0.495936  0.259463  0.422720  0.029717  0.004206   \n",
       "14     0.022819  0.010814  0.495831  0.260762  0.424990  0.029727  0.004212   \n",
       "15     0.022437  0.010777  0.495726  0.260909  0.423908  0.029717  0.004217   \n",
       "16     0.019774  0.010763  0.496041  0.259320  0.420774  0.029690  0.004204   \n",
       "17     0.018254  0.010767  0.496041  0.258018  0.420667  0.029690  0.004204   \n",
       "18     0.016163  0.010725  0.495409  0.258886  0.422937  0.029747  0.004221   \n",
       "19     0.014832  0.010709  0.494141  0.261052  0.424125  0.029791  0.004217   \n",
       "20     0.016923  0.010779  0.495302  0.263796  0.424558  0.029812  0.004220   \n",
       "21     0.018825  0.010804  0.498996  0.266109  0.424990  0.029862  0.004247   \n",
       "22     0.018825  0.010771  0.499948  0.265675  0.424990  0.029906  0.004248   \n",
       "23     0.020156  0.010804  0.498575  0.263509  0.424340  0.029876  0.004225   \n",
       "24     0.023390  0.010868  0.499418  0.263509  0.424125  0.029832  0.004217   \n",
       "25     0.026619  0.010874  0.500474  0.263652  0.424016  0.029815  0.004227   \n",
       "26     0.024910  0.010789  0.497309  0.260331  0.422395  0.029751  0.004219   \n",
       "27     0.018636  0.010713  0.494036  0.257151  0.420667  0.029677  0.004193   \n",
       "28     0.016163  0.010727  0.494036  0.258162  0.421314  0.029684  0.004190   \n",
       "29     0.018254  0.010754  0.494880  0.259897  0.423152  0.029724  0.004210   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14950  0.020156  0.010725  0.495726  0.259173  0.427692  0.029758  0.004221   \n",
       "14951  0.018443  0.010727  0.495302  0.258739  0.428556  0.029747  0.004215   \n",
       "14952  0.019774  0.010771  0.497309  0.261196  0.428663  0.029788  0.004224   \n",
       "14953  0.022630  0.010791  0.498257  0.262641  0.428663  0.029822  0.004231   \n",
       "14954  0.022248  0.010779  0.497201  0.260762  0.428880  0.029815  0.004224   \n",
       "14955  0.020916  0.010769  0.497309  0.260041  0.429312  0.029785  0.004214   \n",
       "14956  0.021298  0.010758  0.497519  0.260331  0.428663  0.029764  0.004212   \n",
       "14957  0.021298  0.010758  0.496148  0.259030  0.427367  0.029771  0.004214   \n",
       "14958  0.022819  0.010775  0.496358  0.258018  0.426610  0.029771  0.004210   \n",
       "14959  0.026812  0.010808  0.496992  0.257297  0.427152  0.029731  0.004201   \n",
       "14960  0.029474  0.010841  0.496675  0.255995  0.426718  0.029690  0.004196   \n",
       "14961  0.029092  0.010829  0.496675  0.255562  0.425529  0.029663  0.004194   \n",
       "14962  0.026241  0.010765  0.496148  0.255274  0.425854  0.029663  0.004190   \n",
       "14963  0.021866  0.010740  0.495619  0.254840  0.425961  0.029670  0.004190   \n",
       "14964  0.020727  0.010775  0.496148  0.254840  0.425639  0.029657  0.004193   \n",
       "14965  0.023579  0.010798  0.496253  0.254984  0.425854  0.029663  0.004189   \n",
       "14966  0.023957  0.010783  0.495831  0.254407  0.425746  0.029687  0.004183   \n",
       "14967  0.023008  0.010771  0.495831  0.253973  0.425529  0.029663  0.004179   \n",
       "14968  0.025099  0.010783  0.496358  0.254407  0.425746  0.029646  0.004177   \n",
       "14969  0.024528  0.010777  0.496675  0.255562  0.425961  0.029694  0.004178   \n",
       "14970  0.021487  0.010756  0.496148  0.255418  0.426071  0.029710  0.004182   \n",
       "14971  0.023390  0.010781  0.496041  0.254840  0.426395  0.029684  0.004178   \n",
       "14972  0.027383  0.010808  0.496570  0.254984  0.424882  0.029650  0.004166   \n",
       "14973  0.026812  0.010795  0.495936  0.254116  0.423584  0.029623  0.004162   \n",
       "14974  0.024150  0.010783  0.494775  0.253685  0.424340  0.029619  0.004166   \n",
       "14975  0.021677  0.010765  0.495092  0.252961  0.423908  0.029606  0.004163   \n",
       "14976  0.020156  0.010732  0.494248  0.250505  0.421856  0.029586  0.004153   \n",
       "14977  0.020916  0.010729  0.493297  0.249927  0.421531  0.029596  0.004161   \n",
       "14978  0.022819  0.010754  0.494775  0.251373  0.423476  0.029626  0.004177   \n",
       "14979  0.026430  0.010800  0.496148  0.251950  0.424990  0.029636  0.004177   \n",
       "\n",
       "        14  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "5      0.0  \n",
       "6      0.0  \n",
       "7      0.0  \n",
       "8      0.0  \n",
       "9      0.0  \n",
       "10     0.0  \n",
       "11     0.0  \n",
       "12     0.0  \n",
       "13     0.0  \n",
       "14     0.0  \n",
       "15     0.0  \n",
       "16     0.0  \n",
       "17     0.0  \n",
       "18     0.0  \n",
       "19     0.0  \n",
       "20     0.0  \n",
       "21     0.0  \n",
       "22     0.0  \n",
       "23     0.0  \n",
       "24     0.0  \n",
       "25     0.0  \n",
       "26     0.0  \n",
       "27     0.0  \n",
       "28     0.0  \n",
       "29     0.0  \n",
       "...    ...  \n",
       "14950  0.0  \n",
       "14951  0.0  \n",
       "14952  0.0  \n",
       "14953  0.0  \n",
       "14954  0.0  \n",
       "14955  0.0  \n",
       "14956  0.0  \n",
       "14957  0.0  \n",
       "14958  0.0  \n",
       "14959  1.0  \n",
       "14960  1.0  \n",
       "14961  1.0  \n",
       "14962  1.0  \n",
       "14963  1.0  \n",
       "14964  1.0  \n",
       "14965  1.0  \n",
       "14966  1.0  \n",
       "14967  1.0  \n",
       "14968  1.0  \n",
       "14969  1.0  \n",
       "14970  1.0  \n",
       "14971  1.0  \n",
       "14972  1.0  \n",
       "14973  1.0  \n",
       "14974  1.0  \n",
       "14975  1.0  \n",
       "14976  1.0  \n",
       "14977  1.0  \n",
       "14978  1.0  \n",
       "14979  1.0  \n",
       "\n",
       "[14980 rows x 15 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize the data\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#training_data = scaler.fit_transform(training_data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(training_data.values)\n",
    "training_data = pd.DataFrame(scaled)\n",
    "training_data\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "training_data_new = training_data.copy()\n",
    "\n",
    "for i in range(window_size):\n",
    "    training_data = pd.concat([training_data, training_data_new.shift(-(i+1))], axis = 1)\n",
    "    \n",
    "training_data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = training_data.drop([14], axis=1).values\n",
    "y = training_data[14].values\n",
    "\n",
    "print(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14970, 154)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14970, 11)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "#train = quantile_transformer.fit_transform(train)\n",
    "#test = quantile_transformer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01056255, 0.23476583, 0.55281474, ..., 0.42369341, 0.02970374,\n",
       "        0.00415909],\n",
       "       [0.01061914, 0.23703369, 0.54859935, ..., 0.42596125, 0.02965316,\n",
       "        0.00422153],\n",
       "       [0.01056424, 0.23476583, 0.55176003, ..., 0.42455755, 0.02960264,\n",
       "        0.00417057],\n",
       "       ...,\n",
       "       [0.01061248, 0.24115926, 0.55500975, ..., 0.4267179 , 0.02965992,\n",
       "        0.00419569],\n",
       "       [0.01050599, 0.22940378, 0.54658069, ..., 0.42250259, 0.02967339,\n",
       "        0.00417272],\n",
       "       [0.01079052, 0.24435397, 0.56633582, ..., 0.43363522, 0.03003385,\n",
       "        0.00426387]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10479, 154, 1)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10479, 11)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115269,)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new = y_train.ravel()\n",
    "y_test_new = y_test.ravel()\n",
    "\n",
    "y_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 2D array into 3D array\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1],1)) \n",
    "x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10479, 154, 1)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115269,)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the LSTM model 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM((1),batch_input_shape=(None,154,1),return_sequences=False))\n",
    "#model.add(LSTM(input_shape=(154,1),return_sequences=False))\n",
    "#model.add(Dense(1))\n",
    "\n",
    "#model.add(Dense(1, activation='s1igmoid'))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#model.compile(optimizer = 'rmsprop',\n",
    "            #  loss = 'mean_squared_error',\n",
    "            #  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 10479 input samples and 115269 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-d029efbf5f10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    802\u001b[0m             ]\n\u001b[0;32m    803\u001b[0m             \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m                 \u001b[1;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    235\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 10479 input samples and 115269 target samples."
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train_new, validation_split=0.33,epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_11_input to have shape (14, 1) but got array with shape (154, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-201-e12689da529c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscores1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_11_input to have shape (14, 1) but got array with shape (154, 1)"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "scores1 = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'test_loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_9_input to have shape (11, 154) but got array with shape (14, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-4770c1308ca8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Use confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#convert ND array to 1D array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \"\"\"\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_9_input to have shape (11, 154) but got array with shape (14, 1)"
     ]
    }
   ],
   "source": [
    "# Use confusion matrix\n",
    "y_pred = model.predict_classes(x_test)\n",
    "\n",
    "y_pred1 = y_pred.ravel() #convert ND array to 1D array\n",
    "\n",
    "pd.crosstab(y_test, y_pred1, rownames=['True'], colnames=['Predicted'], margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-9681b501ad05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred1' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-241-df7e947dcc0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred1' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-dd6bf39167ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Use classification report(precision, recall, f-measure)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Use classification report(precision, recall, f-measure)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
